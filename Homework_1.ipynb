{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Sentiment Analysis with NaÃ¯ve Bayes\n",
    "#### CSCI 3832 Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lemmas and inflected forms, hyponyms/hypernyms, the distributional hypothesis\n",
    "2. Tokenization, vocabularies, and feature extraction for a Naive Bayes model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(1 point) Your name and email here*\n",
    "Blaizun Diamond\n",
    "bldi9852@colorado.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Free Response Questions \n",
    "*(9 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Write down the lemmas of the following inflected forms:**\n",
    "1. walked\n",
    "2. taught\n",
    "3. best\n",
    "4. are\n",
    "5. running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "1. walk\n",
    "2. teach\n",
    "3. good\n",
    "4. be\n",
    "5. run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: Write down 3 hyponyms of the following words:**\n",
    "1. dog\n",
    "2. food\n",
    "3. profession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "1. Corgi\n",
    "2. vegetables\n",
    "3. Mechanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: In your own words, describe:**\n",
    "1. The distributional hypothesis (see lecture on distributional semantics)\n",
    "2. How is the distributional hypothesis relvant to NLP systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "1. The frequency of occurence of one word in relation to other words can give us insights into the sense and sentiment of the word.\n",
    "2. We can train models based on the distribution of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Sentiment Analysis with Naive Bayes \n",
    "*(90 points)*\n",
    "In this section, our goal is to classify a set of movie reviews as positive or negative. For our dataset, we'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). To get started, download the dataset from the link, and extract it to where your notebook is. Next, we'll load the data and look at a couple of examples. \n",
    "\n",
    "*Important: for any project which involves creating or training models, you can **only** do your exploratory data analysis on the training set. Looking at the test set in any way can invalidate your results!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive examples: 12500\n",
      "Number of negative examples: 12500\n",
      "\n",
      "\n",
      "Sample positive example: Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "\n",
      "Sample negative example: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'aclImdb/'\n",
    "\n",
    "pos_train_dir = data_dir + 'train/pos/'\n",
    "neg_train_dir = data_dir + 'train/neg/'\n",
    "\n",
    "def read_folder(folder):\n",
    "    examples = []\n",
    "    for fname in os.listdir(folder):\n",
    "        with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "            examples.append(f.readline().strip())\n",
    "    return examples\n",
    "\n",
    "pos_examples = read_folder(pos_train_dir)\n",
    "neg_examples = read_folder(neg_train_dir)\n",
    "\n",
    "print('Number of positive examples: {}\\nNumber of negative examples: {}\\n\\n'.format(len(pos_examples), len(neg_examples)))\n",
    "\n",
    "print('Sample positive example: {}\\n\\n'.format(pos_examples[0]))\n",
    "print('Sample negative example: {}'.format(neg_examples[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the data, let's create our vocabulary. While we want our vocabulary to cover the whole training set, we'll keep them separate to see if there are any words which are frequently found in one or the other class -- these words might be informative features for classification! \n",
    "\n",
    "The simplest way to create a vocabulary is to split on spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pos_words = []  # A list of all space separated tokens found across all positive examples. (Contains duplicates)\n",
    "neg_words = []\n",
    "\n",
    "pos_vocab = set()  # A list of *unique* separated *types* found across all positive examples. (No duplicates)\n",
    "neg_vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"Teachers\".', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', \"High's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"Teachers\".', 'The', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High.', 'A', 'classic', 'line:', 'INSPECTOR:', \"I'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'STUDENT:', 'Welcome', 'to', 'Bromwell', 'High.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched.', 'What', 'a', 'pity', 'that', 'it', \"isn't!\"]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "(15 points) Your code here. For each class (positive/negative) find both the list of types and tokens for each class. \n",
    "To separate each example into separate words, split the example on spaces. \n",
    "\n",
    "'''\n",
    "print(pos_examples[0])\n",
    "data = pos_examples[0]\n",
    "data = data.split()\n",
    "print(data)\n",
    "for example in pos_examples:\n",
    "    data = example.split(' ')\n",
    "    for word in data:\n",
    "        pos_vocab.add(word)\n",
    "    pos_words += data\n",
    "    \n",
    "for example in neg_examples:\n",
    "    data = example.split(' ')\n",
    "    for word in data:\n",
    "        neg_vocab.add(word)\n",
    "    neg_words += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958696\n",
      "178873\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(len(pos_words))\n",
    "print(len(pos_vocab))\n",
    "\n",
    "assert len(pos_words) == 2958696\n",
    "assert len(pos_vocab) == 178873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets calculate word frequencies for each class. (Hint: use the Python Counter class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pos_frequencies = [] # A list of tuples of the form (word, count). \n",
    "                 # The list should be sorted in descending order, using the count of each tuple as the key\n",
    "\n",
    "neg_frequencies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 148413)\n",
      "('the', 138612)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "''' \n",
    "(15 points) Your code here. For each class (positive/negative) calculate the frequency of each word and save it in pos_counter\n",
    "and neg_counter.\n",
    "\n",
    "Print the top 15 most common word for each class. \n",
    "\n",
    "'''\n",
    "posCounter = Counter(pos_words)\n",
    "negCounter = Counter(neg_words)\n",
    "posLen = len(pos_words)\n",
    "negLen = len(neg_words)\n",
    "for word in pos_vocab:\n",
    "    pos_frequencies.append((word,posCounter[word]))\n",
    "\n",
    "for word in neg_vocab:\n",
    "    neg_frequencies.append((word,negCounter[word]))\n",
    "pos_frequencies.sort(reverse = True, key = lambda a: a[1])\n",
    "neg_frequencies.sort(reverse = True, key = lambda a: a[1])\n",
    "print(pos_frequencies[0])\n",
    "print(neg_frequencies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n",
      "6408\n",
      "('have', 12269)\n",
      "('he', 11765)\n",
      "('be', 11694)\n",
      "('by', 11458)\n",
      "('an', 10793)\n",
      "('one', 10683)\n",
      "('at', 10227)\n",
      "('who', 10150)\n",
      "('from', 10130)\n",
      "('all', 9154)\n",
      "('has', 9030)\n",
      "('her', 8998)\n",
      "('like', 7978)\n",
      "('about', 7828)\n",
      "('very', 7793)\n",
      "('they', 7713)\n",
      "('so', 7374)\n",
      "('or', 7009)\n",
      "('more', 6824)\n",
      "('out', 6692)\n",
      "('some', 6661)\n",
      "('just', 6530)\n",
      "('This', 6315)\n",
      "('when', 5983)\n",
      "('what', 5900)\n",
      "\n",
      "\n",
      "('you', 12704)\n",
      "('his', 11487)\n",
      "('at', 11068)\n",
      "('like', 10155)\n",
      "('they', 10126)\n",
      "('one', 10007)\n",
      "('by', 9966)\n",
      "('he', 9909)\n",
      "('an', 9832)\n",
      "('just', 9795)\n",
      "('or', 9211)\n",
      "('from', 9109)\n",
      "('so', 8958)\n",
      "('all', 8892)\n",
      "('who', 8688)\n",
      "('about', 8458)\n",
      "('out', 7676)\n",
      "('some', 7546)\n",
      "('has', 7442)\n",
      "('her', 6830)\n",
      "('would', 6732)\n",
      "('even', 6506)\n",
      "('no', 6408)\n",
      "('only', 6268)\n",
      "('if', 6167)\n"
     ]
    }
   ],
   "source": [
    "print(posCounter['no'])\n",
    "print(negCounter['no'])\n",
    "for i in range(25,50):\n",
    "    print(pos_frequencies[i])\n",
    "print('\\n')\n",
    "for i in range(25,50):\n",
    "    print(neg_frequencies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert pos_frequencies[0] == ('the', 148413)\n",
    "assert neg_frequencies[0] == ('the', 138612)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 15 words for each class we see two problems:\n",
    "\n",
    "1. The words are essentially the same for each class, which doesn't give us any information on how to differentiate them.\n",
    "2. Look at the most frequent tokens. Are there any tokens which aren't words? Any situations where tokens with different surface forms but the same meaning could be repeated (and if so, how might we control for this?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(5 points) Your answer to 2 here*\n",
    "The line break token is definitely not a word. There are also demostrative pronouns like this and that. To get around this, we could just combine their counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the most frequent words, let's instead look at the most frequent words which explicitly do not appear in the other class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Edie', 82), ('Gundam', 74), ('Antwone', 58), ('/>8/10', 47), ('/>7/10', 46), ('/>10/10', 45), ('Gunga', 44), ('Gypo', 44), ('Din', 43), ('Othello', 41), ('7/10.', 37), ('Blunt', 37), ('Yokai', 37), ('Tsui', 35), ('Blandings', 34), ('Goldsworthy', 32), ('/>9/10', 31), ('Gino', 31), ('Visconti', 30), ('Bernsen', 29), ('Taker', 29), ('Brashear', 29), ('Harilal', 29), ('Clutter', 28), (\"Goldsworthy's\", 27), ('\"Rob', 26), ('Dominick', 25), ('MJ', 25), ('/>7', 24), ('Rosenstrasse', 24), ('Sassy', 24), ('Flavia', 24), ('Ashraf', 23), ('Recommended.', 22), ('Brock', 22), ('vulnerability', 22), ('Sabu', 22), ('Korda', 22), ('Ahmad', 22), ('Stevenson', 22), ('Coop', 22), ('Riff', 22), ('flawless.', 21), ('aunts', 21), (\"Gilliam's\", 21), ('Solo', 21), ('Kells', 21), (\"Capote's\", 21), ('Cutter', 21), ('Blackie', 21)]\n",
      "\n",
      "\n",
      "[('/>4/10', 56), ('/>Avoid', 55), ('2/10', 49), ('*1/2', 45), ('unwatchable.', 43), ('/>3/10', 40), ('Thunderbirds', 40), ('Gamera', 39), ('steaming', 35), ('Wayans', 33), ('Slater', 31), ('drivel.', 30), ('Tashan', 29), ('Aztec', 29), ('/>1/10', 28), ('Sarne', 27), ('Kareena', 26), ('BTK', 26), ('Segal', 26), ('blah,', 26), ('Delia', 26), ('0/10', 25), ('neither.', 25), ('Gram', 25), ('(*1/2)', 24), ('croc', 24), ('Dahmer', 24), ('Darkman', 24), ('Rosanna', 23), ('Zenia', 23), ('tripe.', 22), ('awful!', 22), ('2/10.', 22), ('Kornbluth', 22), ('Saif', 21), ('incoherent,', 21), ('appallingly', 21), ('Shaq', 21), ('Welch', 21), ('Hackenstein', 21), ('/>2/10', 20), ('4/10.', 20), ('kibbutz', 20), ('Clay', 20), ('Morgana', 20), ('\"1\"', 19), ('crawling', 19), ('/>1', 19), ('awfulness', 19), ('Mraovich', 19)]\n"
     ]
    }
   ],
   "source": [
    "only_pos_words = [word for word in pos_words if word not in neg_vocab]\n",
    "only_neg_words = [word for word in neg_words if word not in pos_vocab]\n",
    "\n",
    "opw_counter = Counter(only_pos_words)\n",
    "onw_counter = Counter(only_neg_words)\n",
    "\n",
    "print(opw_counter.most_common()[:50])\n",
    "print('\\n')\n",
    "print(onw_counter.most_common()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin to see some words we would expect to denote a negative review, but not so much for the positive reviews. Why might this be the case? What types of tokens are found in positive reviews but not in negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(5 points) Your answer here*\n",
    "Seems like most of the words that are only found in the positive reviews are names. There are a couple adjectives sprinkled here and there but almost all names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of space separated vocab: 281137\n",
      "['boys:', 'reluctantly,', 'Investigative', 'sex/rape', 'Belushi..he', 'telekenisis', 'she??.', 'insulation.<br', 'Halprin', 'Writing,', 'thugs(1', 'absolute.', 'realize,', 'kills),', 'back..I', '(Nathan', 'Sue,', 'Screweyes', 'dress', 'Wisconsin.', 'candy.', 'Latter-Day', 'rien.\"', 'young', 'immediately)', 'loll', 'persuaded', 'devagan.', 'coined', 'jokes:', 'out?)<br', 'recognize,', 'option.).', 'Shawlee', 'Hitch', 'Galactica,', \"Chandrasekhar's\", 'rival,', \"Tommy's\", 'crocodiles.', 'Dream\".<br', 'Caper,', 'counting.', '\"N.Y.P.D.', 'Fortunately', 'mild,', 'Singapore.', 'effects;', 'written?', '\"repairing\"']\n"
     ]
    }
   ],
   "source": [
    "# Lets now make our combined vocabulary\n",
    "space_vocab = list(pos_vocab.union(neg_vocab))\n",
    "print('Length of space separated vocab: {}'.format(len(space_vocab)))\n",
    "print(space_vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some words from our vocab, what issue do we find by only splitting on spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(5 points) Your answer here*\n",
    "we get a lot of punctuation that isn't filtered out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rather than naively splitting on spaces, we can use tools which are informed about English grammar rules to create a cleaner tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '``', 'Teachers', \"''\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '``', 'Teachers', \"''\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pos_examples_tokenized = [word_tokenize(ex) for ex in pos_examples]\n",
    "neg_examples_tokenized = [word_tokenize(ex) for ex in neg_examples]\n",
    "\n",
    "print(pos_examples_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first example we can see that things like apostrophes, periods, \"n'ts\" and ellipses are better handled.\n",
    "\n",
    "Let's begin defining features for our model. The simplest features are simply if a word exists or not -- however, this is will be very slow if we decide to use the whole vocabulary. Instead, let's create these features for the top 100 most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'and', 'a', 'of', 'to', 'is', '/', '>', '<', 'br', 'in', 'I', 'it', 'that', \"'s\", 'this', 'was', 'The', 'as', 'with', 'movie', 'for', 'film', ')', '(', 'but', \"n't\", \"''\", '``', 'on', 'you', 'are', 'not', 'have', 'his', 'be', 'he', '!', 'one', 'at', 'by', 'all', 'an', 'who', 'they', 'from', 'like', 'It', 'her', 'so', 'or', 'about', 'has', 'just', 'out', '?', 'do', 'This', 'some', 'good', 'more', 'very', 'would', 'what', 'there', 'up', 'can', 'which', 'when', 'time', 'she', 'had', 'if', 'only', 'really', 'story', 'were', 'their', 'even', 'see', 'no', 'my', 'me', 'does', \"'\", 'did', ':', '-', 'than', '...', 'much', 'been', 'could', 'into', 'get', 'will', 'we', 'other']\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_words = [word for ex in pos_examples_tokenized for word in ex] + \\\n",
    "    [word for ex in neg_examples_tokenized for word in ex]\n",
    "\n",
    "atw_counter = Counter(all_tokenized_words)\n",
    "top100 = [tup[0] for tup in atw_counter.most_common(100)] # A list of the top 100 most frequent word\n",
    "\n",
    "print(top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3652\n",
      "6408\n"
     ]
    }
   ],
   "source": [
    "print(posCounter['no'])\n",
    "print(negCounter['no'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following block to define your own features for the NB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 'Gundam')\n",
      "(1.0, 'Edie')\n",
      "(1.0, 'Antwone')\n",
      "(0.9705882352941176, 'Paulie')\n",
      "(0.9629629629629629, 'Sox')\n",
      "(0.9411764705882353, 'Christy')\n",
      "(0.9333333333333333, 'Gandhi')\n",
      "(0.9285714285714286, 'Excellent')\n",
      "(0.9157894736842105, '8/10')\n",
      "(0.9130434782608695, 'Felix')\n",
      "(0.9076923076923077, 'Polanski')\n",
      "(0.9074074074074074, 'Highly')\n",
      "(0.9024390243902439, '7/10')\n",
      "(0.8961038961038961, 'Mildred')\n",
      "(0.8717948717948718, 'flawless')\n",
      "\n",
      "\n",
      "(-1.0, '/>4/10')\n",
      "(-1.0, '/>Avoid')\n",
      "(-0.9813084112149533, 'Seagal')\n",
      "(-0.9787234042553191, 'Boll')\n",
      "(-0.978494623655914, 'Uwe')\n",
      "(-0.974025974025974, '4/10')\n",
      "(-0.9724137931034482, 'Avoid')\n",
      "(-0.9487179487179487, 'pathetic.')\n",
      "(-0.9423076923076923, 'MST3K')\n",
      "(-0.9411764705882353, 'WORST')\n",
      "(-0.9230769230769231, '3/10')\n",
      "(-0.9225806451612903, 'awful.')\n",
      "(-0.92, 'horrible.')\n",
      "(-0.9069767441860465, '1/10')\n",
      "(-0.9029126213592233, 'costs.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"pos = []\\nneg = []\\n\\nprint('\\n')\\nfor word in neg_vocab:\\n    if posCounter[word]-negCounter[word] < 0:\\n        neg.append((posCounter[word]-negCounter[word],word))\\n    else :\\n        pos.append((posCounter[word]-negCounter[word],word))\\n\\npos.sort(reverse = True)\\nneg.sort()\\nfor i in range(15,30):\\n    print(pos[i])\\n\\nprint('\\n')\\nfor i in range(15,30):\\n    print(neg[i]) \""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def num_to_range(num, inMin, inMax, outMin, outMax):\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for word in pos_vocab:\n",
    "    delta = posCounter[word] - negCounter[word]\n",
    "    if delta > 50: #filters out extreme outliers\n",
    "        pos.append((delta/(posCounter[word] + negCounter[word]),word)) #filling an array with the difference in counts of words. A positive difference means weighed towards positive examples\n",
    "pos.sort(reverse = True)\n",
    "for word in neg_vocab:\n",
    "    delta = posCounter[word] - negCounter[word]\n",
    "    if delta < -50: #filters out extreme outliers\n",
    "        neg.append((delta/(posCounter[word] + negCounter[word]),word)) #filling an array with the difference in counts of words. A positive difference means weighed towards positive examples\n",
    "\n",
    "neg.sort()\n",
    "for i in range(15):\n",
    "    print(pos[i])\n",
    "print('\\n')\n",
    "for i in range(15):\n",
    "    print(neg[i])\n",
    "\n",
    "\n",
    "'''pos = []\n",
    "neg = []\n",
    "\n",
    "print('\\n')\n",
    "for word in neg_vocab:\n",
    "    if posCounter[word]-negCounter[word] < 0:\n",
    "        neg.append((posCounter[word]-negCounter[word],word))\n",
    "    else :\n",
    "        pos.append((posCounter[word]-negCounter[word],word))\n",
    "\n",
    "pos.sort(reverse = True)\n",
    "neg.sort()\n",
    "for i in range(15,30):\n",
    "    print(pos[i])\n",
    "\n",
    "print('\\n')\n",
    "for i in range(15,30):\n",
    "    print(neg[i]) '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (25 points) Define features here\n",
    "'''def top100_word_features(example): # 100 features, 1 for each word in the top 100 most frequent words\n",
    "    return {word : 1 if word in example else 0 for word in top100}'''\n",
    "\n",
    "''' Define your own methods here, which take in a single example, and return a feature value (could be a 0/1 truth value, or a count)\n",
    "    Some ideas:\n",
    "        Look at the length of examples, is there a difference between positive and negative examples?\n",
    "        Are there specific words that could be very indicative? They may not be in the top 100. \n",
    "'''\n",
    "\n",
    " # 50.0\n",
    "def word_weight_neg(example): #Delete or modify this \n",
    "   return {word[1] : word[0] if word[1] in example else 0 for word in neg}\n",
    "def word_weight_pos(example):\n",
    "    return {word[1] : word[0] if word[1] in example else 0 for word in pos}\n",
    "\n",
    "def create_feature_dictionary(example):\n",
    "    features = {}\n",
    "    for feat in [word_weight_neg,word_weight_pos]: #Once you've created your methods, and them to this list\n",
    "        features.update(feat(example))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our features for our model, we can create our final dataset, which will consist of extracted features and the example label. \n",
    "\n",
    "We'll also create a *validation* split by taking 20% of the training dataset. Remember, we never use the test set to make modeling decisions (in this case, decisions about features). Experiment with multiple models that make use of different combinations of features. Measure their performance on the validation split to figure out which features are the most helpful (use the show_most_informative_features function). When you've found your final model, evaluate its performance on the held out data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "import random\n",
    "\n",
    "# Convert training examples to a set of features.\n",
    "train = [(create_feature_dictionary(ex), 0) for ex in neg_examples] + \\\n",
    "                [(create_feature_dictionary(ex), 1) for ex in pos_examples]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train)\n",
    "\n",
    "split_percent = .2\n",
    "\n",
    "cutoff = int(split_percent * len(train))\n",
    "\n",
    "validation_set = train[:cutoff]\n",
    "training_set = train[cutoff:]\n",
    "\n",
    "model = NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8572\n",
      "Most Informative Features\n",
      "                   Avoid = -0.9724137931034482      0 : 1      =     54.0 : 1.0\n",
      "                    4/10 = -0.974025974025974      0 : 1      =     37.4 : 1.0\n",
      "                    3/10 = -0.9230769230769231      0 : 1      =     30.6 : 1.0\n",
      "                     Uwe = -0.978494623655914      0 : 1      =     28.4 : 1.0\n",
      "                    7/10 = 0.9024390243902439      1 : 0      =     28.0 : 1.0\n",
      "                  Highly = 0.9074074074074074      1 : 0      =     26.9 : 1.0\n",
      "                  awful. = -0.9225806451612903      0 : 1      =     26.8 : 1.0\n",
      "                   WORST = -0.9411764705882353      0 : 1      =     23.1 : 1.0\n",
      "               Excellent = 0.9285714285714286      1 : 0      =     22.3 : 1.0\n",
      "                    1/10 = -0.9069767441860465      0 : 1      =     21.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print('Validation accuracy: {}'.format(accuracy(model, validation_set)))\n",
    "model.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the sets of features you've considered, and note down the performance of the model with your features. What is the final set of features you found? Walk us through your experimentation process. Did any features work better or worse than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(20 points) Your answer here*\n",
    "I have two features: word_weight_pos and word_weight_neg. These both hinge on my pos and neg lists which are lists of words that had higher positive counts and negative counts respectively. Furthermore the list is a tuple where the second value is a weight, the weight is the difference in count divided by the total count. Also, I filtered out words that had a difference in count of less than 50 because there were quite a few outliers, especially because the vocab wasn't tokenized. Originally I had the weight as just the difference in count and I wanted to map that value to a value between 0 and 1 but I had trouble deciding what the max value would be for the original value range so I switched to this. I also commented  out top_100_word_features because it conflicted with my own methods weight system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test your model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load and process test data\n",
    "pos_test_examples = read_folder(data_dir + 'test/pos/')\n",
    "neg_test_examples = read_folder(data_dir + 'test/neg/')\n",
    "\n",
    "test_set = [(create_feature_dictionary(ex), 0) for ex in neg_test_examples] + \\\n",
    "                [(create_feature_dictionary(ex), 1) for ex in pos_test_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.845\n"
     ]
    }
   ],
   "source": [
    "print('Test set accuracy: {}'.format(accuracy(model, test_set)))\n",
    "\n",
    "# Note that we're looking at accuracy -- this is not always the most reliable metric and other choices like F1 might be more informative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e681adf14836894860de42986132a2fbb5bf9e0a673e28b245b6aa439c639a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
